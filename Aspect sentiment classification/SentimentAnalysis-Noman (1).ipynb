{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspect_term</th>\n",
       "      <th>term_location</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2333_1</td>\n",
       "      <td>Obviously one of the most important features o...</td>\n",
       "      <td>human interface</td>\n",
       "      <td>69--84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1805_1</td>\n",
       "      <td>Good for every day computing and web browsing.</td>\n",
       "      <td>every day computing</td>\n",
       "      <td>9--28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2782_2</td>\n",
       "      <td>while the keyboard itself is alright[comma] th...</td>\n",
       "      <td>mouse command buttons</td>\n",
       "      <td>115--136</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1385_0</td>\n",
       "      <td>Again[comma] the same problem[comma] the right...</td>\n",
       "      <td>right speaker</td>\n",
       "      <td>29--42</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1423_0</td>\n",
       "      <td>My problem was with DELL Customer Service.</td>\n",
       "      <td>DELL Customer Service</td>\n",
       "      <td>20--41</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  example_id                                               text  \\\n",
       "0     2333_1  Obviously one of the most important features o...   \n",
       "1     1805_1     Good for every day computing and web browsing.   \n",
       "2     2782_2  while the keyboard itself is alright[comma] th...   \n",
       "3     1385_0  Again[comma] the same problem[comma] the right...   \n",
       "4     1423_0         My problem was with DELL Customer Service.   \n",
       "\n",
       "             aspect_term  term_location   class  \n",
       "0        human interface         69--84       0  \n",
       "1    every day computing          9--28       1  \n",
       "2  mouse command buttons       115--136      -1  \n",
       "3          right speaker         29--42      -1  \n",
       "4  DELL Customer Service         20--41      -1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "\n",
    "import pandas as pd\n",
    "train_file_path = \"project_2_train/project_2_train/dara 1_train.csv\"\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      3.000000\n",
       "mean     734.333333\n",
       "std      264.258081\n",
       "min      436.000000\n",
       "25%      632.000000\n",
       "50%      828.000000\n",
       "75%      883.500000\n",
       "max      939.000000\n",
       "Name:  class, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_value = train_data[' class'].value_counts(dropna=False)\n",
    "class_value.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    939\n",
       "-1    828\n",
       " 0    436\n",
       "Name:  class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect_term</th>\n",
       "      <th>term_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human interface</td>\n",
       "      <td>69--84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every day computing</td>\n",
       "      <td>9--28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mouse command buttons</td>\n",
       "      <td>115--136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right speaker</td>\n",
       "      <td>29--42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DELL Customer Service</td>\n",
       "      <td>20--41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             aspect_term  term_location\n",
       "0        human interface         69--84\n",
       "1    every day computing          9--28\n",
       "2  mouse command buttons       115--136\n",
       "3          right speaker         29--42\n",
       "4  DELL Customer Service         20--41"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_data.drop([\" class\",\"example_id\",\" text\"],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2   -1\n",
       "3   -1\n",
       "4   -1\n",
       "Name:  class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = train_data[\" class\"]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training bigram tagger\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents()\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def build_backoff_tagger(train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_tagger = build_backoff_tagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9125751765470128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data = train_data[\" text\"]\n",
    "new_text=[]\n",
    "for idx,text_record in enumerate(text_data):\n",
    "    aspect_loc = train_data.loc[idx,\" term_location\"]\n",
    "    locations = aspect_loc.split(\"--\")\n",
    "    current_text = text_data[idx].replace(\"[comma]\", \",\")\n",
    "    #current_text = current_text[:int(locations[0])] + \"<aspect>\" + current_text[int(locations[1]):]\n",
    "    new_text.append([current_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "new_text_aspect=[]\n",
    "for idx,text_record in enumerate(new_text):\n",
    "    aspect_loc = train_data.loc[idx,\" term_location\"]\n",
    "    locations = aspect_loc.split(\"--\")\n",
    "    current_text = text_record[0]\n",
    "    current_text = current_text[:int(locations[0])] + \"<aspect>\" + current_text[int(locations[1]):]\n",
    "    new_text_aspect.append(current_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting words in context 0f 5 from target term\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "sent_list=[]\n",
    "\n",
    "remove_list = [\",\",\".\",\".)\",\"..)\",\").\",\"?\",\"!\"]\n",
    "    \n",
    "    \n",
    "for idx,text_word in enumerate(new_text_aspect):\n",
    "    locations = train_data.loc[idx,\" term_location\"].split(\"--\")\n",
    "    sent = text_word\n",
    "    sent_left = sent[:int(locations[0])]\n",
    "    #token_left = nltk.word_tokenize(sent_left)\n",
    "    #print(token_left)\n",
    "    #bigram_left = nltk.bigrams(token_left)\n",
    "    #bigram_left = *map(' '.join, bigrm)\n",
    "    sent_right = sent[int(locations[0])+8:]\n",
    "    #print(sent_right)\n",
    "    \n",
    "    #token_right = nltk.word_tokenize(sent_right)\n",
    "    if sent_right not in remove_list:\n",
    "        sent_surf = ' '.join(sent_left.split()[-5:])+' '+' '.join(sent_right.split()[:5])\n",
    "    else:\n",
    "        sent_surf = ' '.join(sent_left.split()[-5:])\n",
    "    #print(sent_surf)\n",
    "    sent_list.append([sent_surf])\n",
    "    #bigrm_right = [b for b in nltk.bigrams(token_right)]\n",
    "    #bigrm_right = *map(' '.join, bigrm)\n",
    "    #print(bigrm_right)\n",
    "    #sent_left_words = ngrams(token_left,2)\n",
    "    #sent_right_words = ngrams(sent_right,1)\n",
    "    #words = sent_left_words[-3:] + sent_right_words[:3]\n",
    "    #context_word.append(words)\n",
    "    #print(sent_left_words)\n",
    "#print(context_word)\n",
    "#print(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, WordPunctTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "features=[]\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "for line in sent_list:\n",
    "    #tokenizer = WordPunctTokenizer()\n",
    "    tokens = nltk.word_tokenize(line[0])\n",
    "    #stemmer = PorterStemmer()\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
    "    #bigrams = ngrams(line,2)\n",
    "    for bigram_tuple in bigrams:\n",
    "        x = \"%s %s\" % bigram_tuple\n",
    "        tokens.append(x)\n",
    "    \n",
    "    #result = [b for b in nltk.bigrams(line) if b[0] not in stops and b[1] not in stops]\n",
    "\n",
    "    result = [' '.join([w for w in x.split()]) for x in tokens if x.lower() not in stops and len(x) > 8]\n",
    "    features.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_main=[]\n",
    "for f in features:\n",
    "    if f:\n",
    "        feature_main.append(f)\n",
    "        \n",
    "        \n",
    "vocab_list=[]\n",
    "for f in features:\n",
    "    for ele in f:\n",
    "        vocab_list.append(ele)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "vocab_list_set = set(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_uni=[]\n",
    "stop_list = stopwords.words('english') +['.',',',')','?','!','(','-',':','The','I','It','We',\"'\",'They','My','But','us']\n",
    "\n",
    "stops = set(stop_list)\n",
    "for line in sent_list:\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = nltk.word_tokenize(line[0])\n",
    "    #print(tokens)\n",
    "    #stemmer = PorterStemmer()\n",
    "    #bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    #bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
    "    #bigrams = ngrams(line,2)\n",
    "    #for bigram_tuple in bigrams:\n",
    "    #    x = \"%s %s\" % bigram_tuple\n",
    "    #    tokens.append(x)\n",
    "    \n",
    "    #result = [b for b in nltk.bigrams(line) if b[0] not in stops and b[1] not in stops]\n",
    "    new_token = []\n",
    "    for token in tokens:\n",
    "        if token not in stops:\n",
    "            new_token.append(token)\n",
    "    features_uni.append(new_token)\n",
    "    #if tokens not in stops:\n",
    "    #    features_uni.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list_uni=[]\n",
    "for f in features_uni:\n",
    "    for ele in f:\n",
    "        vocab_list_uni.append(ele)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "vocab_list_set_uni = set(vocab_list_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features_uni\n",
    "#vocab_list_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame({'text':new_text_aspect})\n",
    "#print (text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uni_df = pd.DataFrame(columns =vocab_list_set_uni )\n",
    "import numpy as np\n",
    "uni_df = pd.DataFrame(0, index=np.arange(X.shape[0]), columns =vocab_list_set_uni)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>badly</th>\n",
       "      <th>exposed</th>\n",
       "      <th>secures</th>\n",
       "      <th>Oh</th>\n",
       "      <th>render</th>\n",
       "      <th>Adjust</th>\n",
       "      <th>quiet</th>\n",
       "      <th>Cut</th>\n",
       "      <th>last</th>\n",
       "      <th>frustrating</th>\n",
       "      <th>...</th>\n",
       "      <th>review</th>\n",
       "      <th>save</th>\n",
       "      <th>effeciency</th>\n",
       "      <th>2003</th>\n",
       "      <th>sweet</th>\n",
       "      <th>Unless</th>\n",
       "      <th>AMD</th>\n",
       "      <th>applications</th>\n",
       "      <th>load</th>\n",
       "      <th>similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   badly  exposed  secures  Oh  render  Adjust  quiet  Cut  last  frustrating  \\\n",
       "0      0        0        0   0       0       0      0    0     0            0   \n",
       "1      0        0        0   0       0       0      0    0     0            0   \n",
       "2      0        0        0   0       0       0      0    0     0            0   \n",
       "3      0        0        0   0       0       0      0    0     0            0   \n",
       "4      0        0        0   0       0       0      0    0     0            0   \n",
       "\n",
       "    ...     review  save  effeciency  2003  sweet  Unless  AMD  applications  \\\n",
       "0   ...          0     0           0     0      0       0    0             0   \n",
       "1   ...          0     0           0     0      0       0    0             0   \n",
       "2   ...          0     0           0     0      0       0    0             0   \n",
       "3   ...          0     0           0     0      0       0    0             0   \n",
       "4   ...          0     0           0     0      0       0    0             0   \n",
       "\n",
       "   load  similar  \n",
       "0     0        0  \n",
       "1     0        0  \n",
       "2     0        0  \n",
       "3     0        0  \n",
       "4     0        0  \n",
       "\n",
       "[5 rows x 2350 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(vocabulary=vocab_list_set)\n",
    "X_train_bag = count_vect.fit_transform(text_df[\"text\"])\n",
    "X_term_freq = pd.DataFrame(X_train_bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_bag)\n",
    "#X_train_tfidf.shape\n",
    "X_tfidf = pd.DataFrame(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns with sum less than 25\n",
    "cols_with_less_than_25 = [col for col in X_term_freq.columns \n",
    "                                 if abs(X_term_freq[col].sum()) < 1]\n",
    "reduced_X_train = X_term_freq.drop(cols_with_less_than_25, axis=1)\n",
    "#reduced_X_val  = val_X.drop(cols_with_missing, axis=1)\n",
    "#print(\"Mean Absolute Error from dropping columns with Missing Values:\")\n",
    "#print(getScore(reduced_X_train, train_Y,reduced_X_val,  val_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf[900].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "912"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_less_than_25[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 331)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py:3726: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = op(self.values, np.asarray(other))\n"
     ]
    }
   ],
   "source": [
    "#dropping columns with stop words\n",
    "stop_list = stopwords.words('english')\n",
    "cols_with_stop_words = [col for col in reduced_X_train.columns \n",
    "                                 if reduced_X_train.columns in stop_list ]\n",
    "reduced_X_train_without_stops = reduced_X_train.drop(cols_with_stop_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 331)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_X_train_without_stops.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, WordPunctTokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "features_uni_tagged=[]\n",
    "#stop_list = stopwords.words('english') +['.',',',')','?','!','(','-',':','The','I','It','We',\"'\",'They','My','But','us']\n",
    "\n",
    "#stops = set(stop_list)\n",
    "for line in sent_list:\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = nltk.word_tokenize(line[0])\n",
    "    #print(tokens)\n",
    "    #stemmer = PorterStemmer()\n",
    "    #bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    #bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
    "    #bigrams = ngrams(line,2)\n",
    "    #for bigram_tuple in bigrams:\n",
    "    #    x = \"%s %s\" % bigram_tuple\n",
    "    #    tokens.append(x)\n",
    "    tagged_tokens = ngram_tagger.tag(tokens)\n",
    "    #result = [b for b in nltk.bigrams(line) if b[0] not in stops and b[1] not in stops]\n",
    "    #new_token = []\n",
    "    #for token in tokens:\n",
    "        #if token not in stops:\n",
    "     #       new_token.append(token)\n",
    "    features_uni_tagged.append(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tagged_Sent=[]\n",
    "for feature in features_uni_tagged:\n",
    "    sent = \"\"\n",
    "    for tuples in feature:\n",
    "        sent = sent +\" \" +tuples[0]+\"_\"+tuples[1]\n",
    "    \n",
    "    new_tagged_Sent.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_df_tagged = pd.DataFrame({'text_tagged':new_tagged_Sent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>any_DTI computer_NN is_BEZ the_AT ``_``</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good_JJ for_IN and_CC web_NN browsing_VBG ._.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hollow_JJ sound_NN when_WRB using_VBG the_AT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Again_RB ,_, the_AT same_AP problem_NN ,_, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My_PP$ problem_NN was_BEDZ with_IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         text_tagged\n",
       "0            any_DTI computer_NN is_BEZ the_AT ``_``\n",
       "1      Good_JJ for_IN and_CC web_NN browsing_VBG ._.\n",
       "2       hollow_JJ sound_NN when_WRB using_VBG the_AT\n",
       "3  Again_RB ,_, the_AT same_AP problem_NN ,_, the...\n",
       "4                 My_PP$ problem_NN was_BEDZ with_IN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df_tagged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bag of words with pos tage\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_bag_tagged = count_vect.fit_transform(text_df_tagged[\"text_tagged\"])\n",
    "X_term_freq_tagged = pd.DataFrame(X_train_bag_tagged.toarray(),columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_bag_tagged)\n",
    "#X_train_tfidf.shape\n",
    "X_tfidf_tagged = pd.DataFrame(X_train_tfidf.toarray(),columns=count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns with sum less than 2\n",
    "cols_with_less_than_25 = [col for col in X_tfidf_tagged.columns \n",
    "                                 if abs(X_tfidf_tagged[col].sum()) < 2]\n",
    "reduced_X_train_tagged = X_tfidf_tagged.drop(cols_with_less_than_25, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 539)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_X_train_tagged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_cd</th>\n",
       "      <th>1_cd</th>\n",
       "      <th>2_cd</th>\n",
       "      <th>3_cd</th>\n",
       "      <th>4_cd</th>\n",
       "      <th>7_cd</th>\n",
       "      <th>__nn</th>\n",
       "      <th>_in</th>\n",
       "      <th>_nn</th>\n",
       "      <th>a_at</th>\n",
       "      <th>...</th>\n",
       "      <th>works_vbz</th>\n",
       "      <th>worse_jjr</th>\n",
       "      <th>worst_jjt</th>\n",
       "      <th>worth_jj</th>\n",
       "      <th>would_md</th>\n",
       "      <th>xp_nn</th>\n",
       "      <th>year_nn</th>\n",
       "      <th>you_ppo</th>\n",
       "      <th>you_ppss</th>\n",
       "      <th>your_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10_cd  1_cd  2_cd  3_cd  4_cd  7_cd  __nn  _in  _nn  a_at   ...     \\\n",
       "0    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0   ...      \n",
       "1    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0   ...      \n",
       "2    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0   ...      \n",
       "3    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0   ...      \n",
       "4    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.0  0.0   0.0   ...      \n",
       "\n",
       "   works_vbz  worse_jjr  worst_jjt  worth_jj  would_md  xp_nn  year_nn  \\\n",
       "0        0.0        0.0        0.0       0.0       0.0    0.0      0.0   \n",
       "1        0.0        0.0        0.0       0.0       0.0    0.0      0.0   \n",
       "2        0.0        0.0        0.0       0.0       0.0    0.0      0.0   \n",
       "3        0.0        0.0        0.0       0.0       0.0    0.0      0.0   \n",
       "4        0.0        0.0        0.0       0.0       0.0    0.0      0.0   \n",
       "\n",
       "   you_ppo  you_ppss  your_pp  \n",
       "0      0.0       0.0      0.0  \n",
       "1      0.0       0.0      0.0  \n",
       "2      0.0       0.0      0.0  \n",
       "3      0.0       0.0      0.0  \n",
       "4      0.0       0.0      0.0  \n",
       "\n",
       "[5 rows x 539 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_X_train_tagged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as WordTokenizer\n",
    "\n",
    "def word_tokenizer(data, col):\n",
    "    token=[]\n",
    "    for item in data[col]:\n",
    "         token.append(WordTokenizer(item))\n",
    "\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_token = word_tokenizer(text_df, \"text\")\n",
    "#df.insert(index, 'token_column', token)\n",
    "#print(token)\n",
    "#tokenize_data = pd.DataFrame(token)\n",
    "#word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenize_data.head()\n",
    "#word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/sylvesterraj/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sylvesterraj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'processor',\n",
       " 'is',\n",
       " 'very',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'effective',\n",
       " 'as',\n",
       " 'I',\n",
       " 'load',\n",
       " '<',\n",
       " 'aspect',\n",
       " '>',\n",
       " 'and',\n",
       " 'applications',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words=['aspect','<','>','is','the','of','for','and','.',',']\n",
    "word_token_reduced=[]\n",
    "for token in word_token:\n",
    "    new_token =  [x for x in token if x not in stop_words]\n",
    "    word_token_reduced.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'processor',\n",
       " 'very',\n",
       " 'quick',\n",
       " 'effective',\n",
       " 'as',\n",
       " 'I',\n",
       " 'load',\n",
       " 'applications']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token_reduced[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#polarity shifter\n",
    "negWords = ['no', 'not', 'never', \"n't\"]\n",
    "def polarityShifter(word, index, negList):\n",
    "    if(word in negWords):\n",
    "        negList.append(index + 1)\n",
    "        negList.append(index + 2)\n",
    "    \n",
    "    return negList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sentiWeights(words):\n",
    "    tagged_word = []\n",
    "    tagged = pos_tag(words)\n",
    "    #print(tagged)\n",
    "    #sentiment = 0.0\n",
    "    idx=-1\n",
    "    for word, tag in tagged:\n",
    "        sentiment = 0.0\n",
    "        negIndex = []\n",
    "        prevIndex = -1\n",
    "        idx = idx + 1 \n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV,wn.VERB):\n",
    "            continue\n",
    "\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "        if not lemma:\n",
    "            continue\n",
    "\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "        if not synsets:\n",
    "            continue\n",
    "\n",
    "        # Take the first sense, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        negIndex = polarityShifter(word, idx, negIndex)\n",
    "        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        if(idx in negIndex and prevIndex != idx - 1):\n",
    "            prevIndex = idx\n",
    "            sentiment *= -1\n",
    "        tagged_word.append([word,sentiment,idx])\n",
    "        \n",
    "    return tagged_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_score_word = [] \n",
    "for token in word_token_reduced:\n",
    "    #print(token)\n",
    "    sent_score_word.append(sentiWeights(token))\n",
    "#print(sent_score_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Obviously', 0.5, 0],\n",
       " ['most', 0.0, 2],\n",
       " ['important', 0.875, 3],\n",
       " ['features', 0.0, 4],\n",
       " ['computer', 0.0, 6]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tagged_word\n",
    "sent_score_word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_score_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_token_reduced\n",
    "def getScoreForSentiments(feature_columns_sentiword,sent_score_word):\n",
    "    my_df= []\n",
    "    for senti in sent_score_word:\n",
    "        number_of_pos = 0\n",
    "        number_of_neg = 0\n",
    "        sum_of_scores = 0\n",
    "        maximum_token_score = 0\n",
    "        minimum_token_score = 0\n",
    "        for se in senti:\n",
    "            if se[1] > 0:\n",
    "                number_of_pos = number_of_pos+1\n",
    "\n",
    "            if se[1] < 0:\n",
    "                number_of_neg = number_of_neg+1\n",
    "\n",
    "            sum_of_scores = sum_of_scores + se[1]\n",
    "\n",
    "            if se[1] > maximum_token_score:\n",
    "                maximum_token_score = se[1]\n",
    "\n",
    "            if se[1] < minimum_token_score:\n",
    "                minimum_token_score = se[1]\n",
    "\n",
    "        #print(number_of_pos)\n",
    "\n",
    "        d = {\n",
    "            feature_columns_sentiword[0] : number_of_pos,  \n",
    "            feature_columns_sentiword[1] : number_of_neg,\n",
    "            feature_columns_sentiword[2] : sum_of_scores,\n",
    "            feature_columns_sentiword[3] : maximum_token_score,\n",
    "            feature_columns_sentiword[4] : minimum_token_score\n",
    "        }\n",
    "        my_df.append(d)\n",
    "\n",
    "    my_df = pd.DataFrame(my_df)\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_df_senti' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-535cc1a91c4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_df_senti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_df_senti' is not defined"
     ]
    }
   ],
   "source": [
    "#my_df_senti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bing liu's opinion lexicon - negative words\n",
    "f = open('opinion-lexicon-English/negative-words.txt', 'r',encoding = \"ISO-8859-1\")\n",
    "negative_words_liu = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('opinion-lexicon-English/positive-words.txt', 'r',encoding = \"ISO-8859-1\")\n",
    "positive_words_liu = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#negative_words_liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiWeightsLiu(words):\n",
    "    tagged_word = []\n",
    "    tagged = pos_tag(words)\n",
    "    #print(tagged)\n",
    "    #sentiment = 0.0\n",
    "    idx=-1\n",
    "    for word, tag in tagged:\n",
    "        sentiment = 0.0\n",
    "        negIndex = []\n",
    "        prevIndex = -1\n",
    "        idx = idx + 1\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV,wn.VERB):\n",
    "            continue\n",
    "\n",
    "        if word in positive_words_liu:\n",
    "            sentiment = 1.0\n",
    "        if word in negative_words_liu:\n",
    "            sentiment = -1.0\n",
    "            \n",
    "        negIndex = polarityShifter(word, idx, negIndex)\n",
    "\n",
    "        if(idx in negIndex and prevIndex != idx - 1):\n",
    "            prevIndex = idx\n",
    "            sentiment *= -1\n",
    "            \n",
    "        tagged_word.append([word,sentiment,idx])\n",
    "        \n",
    "    return tagged_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assigning weights on basis of liu's opinion lexicon\n",
    "\n",
    "sent_score_word_liu = [] \n",
    "for token in word_token_reduced:\n",
    "    #print(token)\n",
    "    sent_score_word_liu.append(sentiWeightsLiu(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good', 0.0, 0], ['web', 0.0, 1], ['browsing', 0.0, 2]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_word_liu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aspect_locations_tokens = []\n",
    "for word_ in word_token:\n",
    "    aspect_locations_tokens.append(word_.index(\"aspect\")-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_score_word_liu_new = []\n",
    "for idx,sent_liu in enumerate(sent_score_word_liu):\n",
    "    start_range = aspect_locations_tokens[idx]-3\n",
    "    end_range = aspect_locations_tokens[idx]+3\n",
    "    sent_new =[]\n",
    "    for sent in sent_liu:\n",
    "        if sent[2] >= start_range and sent[2] <= end_range:\n",
    "            sent[1] = sent[1] * 2\n",
    "        sent_new.append(sent)\n",
    "    sent_score_word_liu_new.append(sent_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_score_word_new = []\n",
    "for idx,sent_senti in enumerate(sent_score_word):\n",
    "    start_range = aspect_locations_tokens[idx]-3\n",
    "    end_range = aspect_locations_tokens[idx]+3\n",
    "    sent_new =[]\n",
    "    for sent in sent_senti:\n",
    "        if sent[2] >= start_range and sent[2] <= end_range:\n",
    "            sent[1] = sent[1] * 2\n",
    "        sent_new.append(sent)\n",
    "    sent_score_word_new.append(sent_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Obviously', 0.5, 0],\n",
       " ['most', 0.0, 2],\n",
       " ['important', 0.875, 3],\n",
       " ['features', 0.0, 4],\n",
       " ['computer', 0.0, 6]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_word_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['keyboard', 0.0, 1],\n",
       " ['alright', 0.0, 3],\n",
       " ['plate', 0.0, 4],\n",
       " ['cheap', -1.0, 7],\n",
       " ['plastic', 0.0, 8],\n",
       " ['makes', 0.0, 9],\n",
       " ['hollow', -1.0, 11],\n",
       " ['sound', 0.0, 12],\n",
       " ['using', 0.0, 14]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_word_liu_new[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['keyboard', 0.0, 1],\n",
       " ['alright', 0.0, 3],\n",
       " ['plate', 0.0, 4],\n",
       " ['cheap', -1.0, 7],\n",
       " ['plastic', 0.0, 8],\n",
       " ['makes', 0.0, 9],\n",
       " ['hollow', -1.0, 11],\n",
       " ['sound', 0.0, 12],\n",
       " ['using', 0.0, 14]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_word_liu[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'mpqa/mpqa.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-9474fba3c3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#mpqa lexicon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mpqa/mpqa.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmpqa_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mpqa/mpqa.txt'"
     ]
    }
   ],
   "source": [
    "#mpqa lexicon\n",
    "f = open('mpqa/mpqa.txt', 'r',encoding = \"ISO-8859-1\")\n",
    "mpqa_lines = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mpqa_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-4ab3bc8dcba7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmpqa_sent_score_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmpqa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmpqa_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmpqas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmpqa_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpqas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmpqa_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpqa_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mpqa_lines' is not defined"
     ]
    }
   ],
   "source": [
    "mpqa_sent_score_word = []\n",
    "for mpqa in mpqa_lines:\n",
    "    mpqas = mpqa.split(\" \")\n",
    "    mpqa_types = mpqas[0].split(\"=\")\n",
    "    mpqa_type = mpqa_types[1]\n",
    "    mpqa_words = mpqas[2].split(\"=\")\n",
    "    mpqa_word = mpqa_words[1]\n",
    "    #print(mpqas)\n",
    "    #print(mpqas[5])\n",
    "    mpqa_polarities = mpqas[5].split(\"=\")\n",
    "    mpqa_polarity = mpqa_polarities[1]\n",
    "    score = 0.0\n",
    "    if mpqa_polarity == \"negative\":\n",
    "        if mpqa_type == \"weaksubj\":\n",
    "            score = -0.5\n",
    "        else:\n",
    "            score = -1.0\n",
    "            \n",
    "    if mpqa_polarity == \"positive\":\n",
    "        if mpqa_type == \"weaksubj\":\n",
    "            score = 0.5\n",
    "        else:\n",
    "            score = 1.0\n",
    "    \n",
    "    mpqa_sent_score_word.append([mpqa_word,score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-dd519fa83aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmpqa_sent_score_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "mpqa_sent_score_word[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for mpqa_w in mpqa_sent_score_word:\n",
    "    if 'be' == mpqa_w[0]:\n",
    "        print(mpqa_w[1])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiWeightsMpqa(words):\n",
    "    tagged_word = []\n",
    "    tagged = pos_tag(words)\n",
    "    #print(tagged)\n",
    "    \n",
    "    idx=-1\n",
    "    for word, tag in tagged:\n",
    "        sentiment = 0.0\n",
    "        negIndex = []\n",
    "        prevIndex = -1\n",
    "        idx = idx + 1\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV,wn.VERB):\n",
    "            continue\n",
    "\n",
    "        for mpqa_w in mpqa_sent_score_word:\n",
    "            if word == mpqa_w[0]:\n",
    "                sentiment = mpqa_w[1]\n",
    "                break\n",
    "        \n",
    "        negIndex = polarityShifter(word, idx, negIndex)\n",
    "\n",
    "        if(idx in negIndex and prevIndex != idx - 1):\n",
    "            prevIndex = idx\n",
    "            sentiment *= -1\n",
    "            \n",
    "        tagged_word.append([word,sentiment,idx])\n",
    "        \n",
    "    return tagged_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_score_word_mpqa = [] \n",
    "for token in word_token_reduced:\n",
    "    #print(token)\n",
    "    sent_score_word_mpqa.append(sentiWeightsMpqa(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['problem', 0.0, 1], ['was', 0.0, 2]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_score_word_mpqa[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# populating unigram dataframe with weighted sentiment scores of mpqa\n",
    "\n",
    "for idx,sent_word_list in enumerate(sent_score_word_mpqa):\n",
    "    for sent_word in sent_word_list:\n",
    "        if sent_word[0] in uni_df.columns:\n",
    "            uni_df.loc[idx,sent_word[0]] = sent_word[1]\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# populating unigram dataframe with weighted sentiment scores of liu\n",
    "\n",
    "for idx,sent_word_list in enumerate(sent_score_word_liu):\n",
    "    for sent_word in sent_word_list:\n",
    "        if sent_word[0] in uni_df.columns:\n",
    "            uni_df.loc[idx,sent_word[0]] = uni_df.loc[idx,sent_word[0]] + sent_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# populating unigram dataframe with weighted sentiment scores of sentiword\n",
    "\n",
    "for idx,sent_word_list in enumerate(sent_score_word_new):\n",
    "    for sent_word in sent_word_list:\n",
    "        if sent_word[0] in uni_df.columns:\n",
    "            uni_df.loc[idx,sent_word[0]] = (uni_df.loc[idx,sent_word[0]] + sent_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 2350)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_df = uni_df.add_suffix('_without_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced_uni_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-9eaee33670c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreduced_uni_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduced_uni_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_without_tag'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reduced_uni_df' is not defined"
     ]
    }
   ],
   "source": [
    "reduced_uni_df = reduced_uni_df.add_suffix('_without_tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Amazon unigram lexicon\n",
    "\n",
    "f = open('opinion-lexicon-English/Amazon-laptops-electronics-reviews-unigrams.txt', 'r',encoding = \"ISO-8859-1\")\n",
    "domain_specific_lex = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Yelp unigram lexicon\n",
    "f = open('opinion-lexicon-English/Yelp-restaurant-reviews-unigrams.txt', 'r',encoding = \"ISO-8859-1\")\n",
    "domain_specific_lex = f.read().splitlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex_score_list = {}\n",
    "lex_list = []\n",
    "for lexes in domain_specific_lex:\n",
    "        splitted_lex = lexes.split(',')\n",
    "        lex_score_list[splitted_lex[0]] = splitted_lex[1]\n",
    "        lex_list.append(splitted_lex[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiWeightsLex(words):\n",
    "    tagged_word = []\n",
    "    #tagged = pos_tag(words)\n",
    "    #print(tagged)\n",
    "    idx=-1\n",
    "   \n",
    "    prevIndex = -1\n",
    "    \n",
    "    #for word, tag in tagged:\n",
    "        #wn_tag = penn_to_wn(tag)\n",
    "        #if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV,wn.VERB):\n",
    "            #continue\n",
    "    for word in words:\n",
    "        sentiment = 0.0\n",
    "        negIndex = []\n",
    "        idx = idx + 1\n",
    "        if word in lex_list:\n",
    "            sentiment = float(lex_score_list[word])\n",
    "        \n",
    "        negIndex = polarityShifter(word, idx, negIndex)\n",
    "\n",
    "        if(idx in negIndex and prevIndex != idx - 1):\n",
    "            prevIndex = idx\n",
    "            sentiment *= -1\n",
    "            #print(word, sentiment, idx)\n",
    "            \n",
    "        tagged_word.append([word,sentiment,idx])\n",
    "        \n",
    "    return tagged_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti_score_lex = [] \n",
    "for token in word_token_reduced:\n",
    "    senti_score_lex.append(sentiWeightsLex(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_score_lex[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bit of modification\n",
    "\n",
    "def getScoreForSentimentsLex(feature_columns_sentiword,sent_score_word):\n",
    "    my_df= []\n",
    "    for senti in sent_score_word:\n",
    "        total_pos = 0\n",
    "        total_neg = 0\n",
    "        sum_of_scores = 0\n",
    "        maximum_token_score = 0\n",
    "        minimum_token_score = 0\n",
    "        for se in senti:\n",
    "            if se[1] > 0:\n",
    "                total_pos += se[1]\n",
    "\n",
    "            if se[1] < 0:\n",
    "                total_neg += se[1]\n",
    "\n",
    "            sum_of_scores = sum_of_scores + se[1]\n",
    "\n",
    "            if se[1] > maximum_token_score:\n",
    "                maximum_token_score = se[1]\n",
    "\n",
    "            if se[1] < minimum_token_score:\n",
    "                minimum_token_score = se[1]\n",
    "\n",
    "        #print(number_of_pos)\n",
    "\n",
    "        d = {\n",
    "            feature_columns_sentiword[0] : total_pos,  \n",
    "            feature_columns_sentiword[1] : total_neg,\n",
    "            feature_columns_sentiword[2] : sum_of_scores,\n",
    "            feature_columns_sentiword[3] : maximum_token_score,\n",
    "            feature_columns_sentiword[4] : minimum_token_score\n",
    "        }\n",
    "        my_df.append(d)\n",
    "\n",
    "    my_df = pd.DataFrame(my_df)\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# populating unigram dataframe with weighted sentiment scores of sentiword\n",
    "\n",
    "for idx,sent_word_list in enumerate(senti_score_lex):\n",
    "    for sent_word in sent_word_list:\n",
    "        if sent_word[0] in uni_df.columns:\n",
    "            uni_df.loc[idx,sent_word[0]] = (uni_df.loc[idx,sent_word[0]] + sent_word[1])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dropping columns with absolute sum less than 0.001\n",
    "cols_with_less_than_abs_sum = [col for col in uni_df.columns \n",
    "                                 if abs(uni_df[col].sum()) < 2]\n",
    "reduced_uni_df = uni_df.drop(cols_with_less_than_abs_sum, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_uni_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns_sentiword_lex = ['total positives lex','total negatives lex','sum of scores lex','maximum token score lex','minimum token score lex']\n",
    "\n",
    "\n",
    "my_df_lex = getScoreForSentimentsLex(feature_columns_sentiword_lex,senti_score_lex)\n",
    "\n",
    "my_df_lex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns_sentiword = ['number of positives','number of negatives','sum of scores','maximum token score','minimum token score']\n",
    "\n",
    "my_df_senti = getScoreForSentiments(feature_columns_sentiword,sent_score_word_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_score_word_liu_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-f5295aa1719c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmy_df_liu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetScoreForSentiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns_sentiword_liu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_score_word_liu_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_score_word_liu_new' is not defined"
     ]
    }
   ],
   "source": [
    "feature_columns_sentiword_liu = ['number of positives liu','number of negatives liu','sum of scores liu','maximum token score liu','minimum token score liu']\n",
    "\n",
    "\n",
    "my_df_liu = getScoreForSentiments(feature_columns_sentiword_liu,sent_score_word_liu_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sent_score_word_mpqa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8391de770a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmy_df_mpqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetScoreForSentiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_columns_sentiword_mpqa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_score_word_mpqa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sent_score_word_mpqa' is not defined"
     ]
    }
   ],
   "source": [
    "feature_columns_sentiword_mpqa = ['number of positives mpqa','number of negatives mpqa','sum of scores mpqa','maximum token score mpqa','minimum token score mpqa']\n",
    "\n",
    "\n",
    "my_df_mpqa = getScoreForSentiments(feature_columns_sentiword_mpqa,sent_score_word_mpqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_df_liu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-88198880ba07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_df_liu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_df_liu' is not defined"
     ]
    }
   ],
   "source": [
    "my_df_liu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list_senti = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9--28'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.loc[1,\" term_location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data[\" text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_text_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2203"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2063"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_main) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5752"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vocab_list_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vocab_list_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cv = CountVectorizer(vocabulary=vocab_list_set)\n",
    "#cv.fit_transform(train_data[\" text\"]).toarray()\n",
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8507"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 5752)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_term_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5742</th>\n",
       "      <th>5743</th>\n",
       "      <th>5744</th>\n",
       "      <th>5745</th>\n",
       "      <th>5746</th>\n",
       "      <th>5747</th>\n",
       "      <th>5748</th>\n",
       "      <th>5749</th>\n",
       "      <th>5750</th>\n",
       "      <th>5751</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5752 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   5742  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   5743  5744  5745  5746  5747  5748  5749  5750  5751  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 5752 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_term_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203, 331)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for idx,ngrm in enumerate(ngram_set):\n",
    "#    print('ngram: {0}\\n'.format(ngrm))\n",
    "#    print('ngram.shape: {0}'.format(ngrm.shape))\n",
    "#    print('vectorizer.vocabulary_: {0}'.format(vectorizers[idx].vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_df_liu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-7bc4ee39beaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_df_liu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df_senti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_df_liu' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_mix =[]\n",
    "train_data_mix = my_df_liu.join(my_df_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data_mix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_df_senti' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-497c76892cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduced_X_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df_senti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_df_senti' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_mix = reduced_X_train.join(my_df_senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-a10cee7e664d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df_liu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "train_data_mix = train_data_mix.join(my_df_liu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-e9cddeceed96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df_mpqa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "train_data_mix = train_data_mix.join(my_df_mpqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reduced_uni_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-4d264e6a30d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreduced_uni_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reduced_uni_df' is not defined"
     ]
    }
   ],
   "source": [
    "reduced_uni_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-7527e940a618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_uni_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_mix = train_data_mix.join(reduced_uni_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-3ae59baa49ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df_lex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "train_data_mix = train_data_mix.join(my_df_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-3579db68e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_data_mix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-69c5e7be4ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_X_train_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'join'"
     ]
    }
   ],
   "source": [
    "train_data_mix = train_data_mix.join(reduced_X_train_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 2203]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-9558dedcb502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mexample_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'example_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 2203]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(train_data_mix, Y,random_state = 0)\n",
    "example_ids = val_X['example_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=683, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest train\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=683, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "forest_model.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7259528130671506"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "forest_pred = forest_model.predict(val_X)\n",
    "accuracy_score(val_Y, forest_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.69186603, 0.71084337, 0.78729839]), array([0.87318841, 0.2706422 , 0.83173589]), array([0.77202349, 0.39202658, 0.8089073 ]), array([828, 436, 939]))\n",
      "Overal Accuracy\n",
      "0.7362687244666364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#Random Forest using cross validation and confusion matrix\n",
    "forest_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=683, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "predicted = cross_val_predict(forest_model, train_data_mix, Y, cv=10)\n",
    "conf_mat_forest = confusion_matrix(Y,predicted,labels=[-1,0,1])\n",
    "#calclulateF1(conf_mat_forest)\n",
    "#print(calclulateF1(conf_mat_naive))\n",
    "print(precision_recall_fscore_support(Y,predicted))\n",
    "print(\"Overal Accuracy\")\n",
    "print(accuracy_score(Y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-338-62e7f7e0d2bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mxgboost_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgboost_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model)\u001b[0m\n\u001b[1;32m    504\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 898\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgboost = XGBClassifier(learning_rate= 0.2, n_estimators= 300,max_depth=30)\n",
    "xgboost.fit(train_X,train_Y)\n",
    "xgboost_pred = xgboost.predict(val_X)\n",
    "accuracy_score(val_Y, xgboost_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7332123411978222"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=99999,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'),\n",
    "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
    "         max_samples=1.0, n_estimators=99, n_jobs=1, oob_score=False,\n",
    "         random_state=None, verbose=0, warm_start=False)\n",
    "clf2 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=683, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "clf3 = MLPClassifier()\n",
    "\n",
    "vote_model = VotingClassifier(estimators=[('bag', clf1), ('rf', clf2), ('mlp', clf3)], voting='hard')\n",
    "vote_model.fit(train_X,train_Y)\n",
    "vote_pred = vote_model.predict(val_X)\n",
    "accuracy_score(val_Y, vote_pred)\n",
    "def genOutput(\"Noman_Mulla_Sylvester_Raj_Data-1.txt\",example_ids,vote_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.7110885 , 0.62992126, 0.78674948]), array([0.8442029 , 0.36697248, 0.80937167]), array([0.7719492 , 0.46376812, 0.79790026]), array([828, 436, 939]))\n",
      "Overal Accuracy\n",
      "0.7349069450748978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "clf1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=99999,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'),\n",
    "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
    "         max_samples=1.0, n_estimators=49, n_jobs=1, oob_score=False,\n",
    "         random_state=None, verbose=0, warm_start=False)\n",
    "clf2 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=683, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "clf3 = MLPClassifier()\n",
    "vote_model = VotingClassifier(estimators=[('bag', clf1), ('rf', clf2), ('mlp', clf3)], voting='hard')\n",
    "predicted = cross_val_predict(vote_model, train_data_mix, Y, cv=10)\n",
    "conf_mat_forest = confusion_matrix(Y,predicted,labels=[-1,0,1])\n",
    "#calclulateF1(conf_mat_forest)\n",
    "#print(calclulateF1(conf_mat_naive))\n",
    "print(precision_recall_fscore_support(Y,predicted))\n",
    "print(\"Overal Accuracy\")\n",
    "print(accuracy_score(Y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svm grid search hyper parameter tuning\n",
    "\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    # \n",
    "    Cs = [0.01,0.1,1.5, 3]\n",
    "    #\n",
    "    gammas = [0.001, 0.003,0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    #grid_search.best_params_\n",
    "    #means = clf.cv_results_['mean_test_score']\n",
    "    #stds = clf.cv_results_['std_test_score']\n",
    "    \n",
    "    print(\"Best Params\")\n",
    "    print(grid_search.best_params_)\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params\n",
      "{'C': 0.01, 'gamma': 0.001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'gamma': 0.001}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_param_selection(train_data_mix, Y,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9309927360774818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6950998185117967"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Library\n",
    "from sklearn import svm\n",
    "#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset\n",
    "# Create SVM classification object \n",
    "# C = [3.1,3.5,3.55,3.981,3.764,3.333]\n",
    "# gamma = [0.01,0.03,0.05,0.055,0.066,0.077,0.091,0.092,0.0933,0.1]\n",
    "model = svm.SVC(kernel='rbf', C=3, gamma=0.1) \n",
    "# there is various option associated with it, like changing kernel, gamma and C value. Will discuss more # about it in next section.Train the model using the training sets and check score\n",
    "model.fit(train_X, train_Y)\n",
    "print(model.score(train_X, train_Y))\n",
    "#Predict Output\n",
    "predicted= model.predict(val_X)\n",
    "accuracy_score(val_Y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.55503876, 0.34291581, 0.76032389]), array([0.4447205 , 0.26382306, 0.86783734]), array([0.4937931 , 0.29821429, 0.81053086]), array([ 805,  633, 2164]))\n",
      "Overal Accuracy\n",
      "0.667129372570794\n"
     ]
    }
   ],
   "source": [
    "svm_model = svm.SVC(kernel='rbf', C=3.5, gamma=0.01) \n",
    "predicted = cross_val_predict(svm_model, train_data_mix, Y, cv=10)\n",
    "conf_mat_forest = confusion_matrix(Y,predicted,labels=[-1,0,1])\n",
    "#calclulateF1(conf_mat_forest)\n",
    "#print(calclulateF1(conf_mat_naive))\n",
    "print(precision_recall_fscore_support(Y,predicted))\n",
    "print(\"Overal Accuracy\")\n",
    "print(accuracy_score(Y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def genOutput(output_file, example_ids, predicted):\n",
    "    for i in range(0, len(example_ids)):\n",
    "        out_file.write(example_ids[i] \";;\"+ predicted[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
